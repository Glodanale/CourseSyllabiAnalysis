{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combo PDF Extractor\n",
    "\n",
    "This file's purpose is to extract text from the course syllabi PDFs as accurately as possible into text files for further processing in the project.\n",
    "\n",
    "Please note that the pytesseract library relies on installation of tesseract which is from the link provided below:\n",
    "\n",
    "https://github.com/h/pytesseract\n",
    "\n",
    "\n",
    "### Input:  UniqueCourses folder\n",
    "### Output:  TextFiles_Combo folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pdfplumber\n",
    "from pdf2image import convert_from_path\n",
    "from pytesseract import pytesseract\n",
    "from difflib import get_close_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Tesseract path (if needed)\n",
    "pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "pdf_folder = './UniqueCourses'      # Folder where the pdfs you need is stored\n",
    "output_folder = './TextFiles_Combo' # Folder where the text files will be stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization function (simple split by whitespace)\n",
    "def tokenize(text):\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct token using the closest match from pdfp_text\n",
    "def correct_token(token, reference_tokens):\n",
    "    matches = get_close_matches(token, reference_tokens, n=1, cutoff=0.8)  # Adjust cutoff for similarity\n",
    "    if matches:                                 # If there is at least one token that meets the similarity requirement\n",
    "        reference_tokens.remove(matches[0])     # Remove first instance of token that is likely the correct version of the typo from our reference text to prevent duplicate matching later on\n",
    "        return matches[0]                       # Return first instance of token that is likely the correct version of the typo token\n",
    "    else:\n",
    "        # !!! Comment one of these two return lines depending on your needs\n",
    "        return token    # Return original token if no close match is found      (keep possible/likely garbage, good if pytesseract consistently misinterprets a specific token\n",
    "                                                                                # with a different token that is not similar using this similarity equation)\n",
    "                                                                                \n",
    "        #return \"\"       # Return an empty string if no close match is found     (discard possible/likely garbage, good if pytesseract halucinates a lot of useless garbage\n",
    "                                                                                # but the typos are consistently similar and easily corrected using this similarity equation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through all PDF files in the folder\n",
    "for filename in os.listdir(pdf_folder):\n",
    "    if filename.endswith(\".pdf\"):\n",
    "        pdf_path = os.path.join(pdf_folder, filename)\n",
    "        text_filename = os.path.splitext(filename)[0] + \".txt\"\n",
    "        text_path = os.path.join(output_folder, text_filename)\n",
    "\n",
    "        full_corrected_text = \"\"\n",
    "\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for page_number, page in enumerate(pdf.pages, start=1):\n",
    "                # Step 1: Extract pytesseract text\n",
    "                # This is the resource that is very good at grouping text correctly but generates typos and hallucinations\n",
    "                image = convert_from_path(pdf_path, first_page=page_number, last_page=page_number)[0]\n",
    "                pyt_text = pytesseract.image_to_string(image, lang='eng')\n",
    "                pyt_tokens = tokenize(pyt_text)\n",
    "\n",
    "                # Step 2: Extract pdfplumber text\n",
    "                # This is the resource that extracts text almost perfectly but reads everything as single column horizontal\n",
    "                # with no awareness of alternative text grouping styles\n",
    "                pdfp_text = page.extract_text()\n",
    "                if not pdfp_text:\n",
    "                    print(f\"No text found on page {page_number} using pdfplumber.\")\n",
    "                    continue\n",
    "                pdfp_tokens = tokenize(pdfp_text)\n",
    "\n",
    "                # Step 3: Sequential token correction\n",
    "                corrected_tokens = []\n",
    "                for token in pyt_tokens:                        # Iterate through the tokenized text that is correctly grouped but contains typos\n",
    "                    if token in pdfp_tokens:                    # If an exact match of the token can be found in the good text extraction, this likely is not an artificial typo\n",
    "                        corrected_tokens.append(token)          # and should be added to the list of tokens for this file page\n",
    "                        pdfp_tokens.remove(token)               # Remove matched token in our reference text file to reduce the chance of redundant matches\n",
    "                    else:                                       # Token is likely an artificial typo\n",
    "                        corrected_tokens.append(correct_token(token, pdfp_tokens))  # Append the corrected version of the typo token if found, otherwise keep as-is\n",
    "\n",
    "                # Step 4: Reconstruct corrected text\n",
    "                full_corrected_text += \" \".join(corrected_tokens) + \"\\n\"        # Add cleaned text from page to our running text string\n",
    "\n",
    "        # Write the final corrected text to a file\n",
    "        with open(text_path, 'w', encoding='utf-8') as text_file:\n",
    "            text_file.write(full_corrected_text)                                # Write our pdf extraction to a new text file and store it in out output folder\n",
    "\n",
    "        print(f\"Processed and saved: {text_filename}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
